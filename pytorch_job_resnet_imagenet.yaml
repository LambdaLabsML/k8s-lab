apiVersion: "kubeflow.org/v1"
kind: "PyTorchJob"
metadata:
  name: "pytorch-job-resnet-imagenet"
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          tolerations:
            - key: nvidia.com/gpu
              value: "true"
              operator: Equal
              effect: NoSchedule
          containers:
            - name: pytorch
              image: chuanli11/pytorch-ngc:1.28
              # args: ["-c", "while true; do sleep 3600; done"]
              args: ["--backend", "nccl", "--batch-size", "256", "--epochs", "4", "--repeat", "1", "--num-workers", "16", "--pin-memory", "--persistent-workers", "--prefetch-factor", "2"]
              # args: ["--backend", "nccl", "--batch-size", "256", "--epochs", "4", "--repeat", "1", "--num-workers", "2", "--use-syn", "--num-syn-batches", "100"]
              resources: 
                limits:
                  nvidia.com/gpu: 1
                  rdma/rdma_shared_device_a: 1
                  memory: "128Gi" 
                  # cpu: 256
                requests:
                  nvidia.com/gpu: 1
                  rdma/rdma_shared_device_a: 1
                  memory: "128Gi"
                  cpu: 16
              command:
                # - "/bin/bash"
                - "python"
                - "/opt/workspace/src/benchmark_resnet_imagenet.py"
              env:
                - name: NCCL_DEBUG
                  value: "INFO"
                - name: NCCL_IB_PCI_RELAXED_ORDERING
                  value: "0"
                - name: NCCL_IB_DISABLE
                  value: "0"
                - name: NCCL_IB_HCA
                  value: "mlx5_0,mlx5_1,mlx5_2,mlx5_5,mlx5_6,mlx5_7,mlx5_8,mlx5_13"
              securityContext:
                capabilities:
                  add:
                    - IPC_LOCK
              volumeMounts:
                - name: shared-storage
                  mountPath: /opt/data
                - mountPath: /dev/shm
                  name: dshm
          volumes:
            - name: shared-storage
              persistentVolumeClaim:
                claimName: pvc-ml-intelliflash
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: "128Gi"
    Worker:
      replicas: 63
      restartPolicy: OnFailure
      template:
        spec:
          tolerations:
            - key: nvidia.com/gpu
              value: "true"
              operator: Equal
              effect: NoSchedule
          containers: 
            - name: pytorch
              image: chuanli11/pytorch-ngc:1.28
              # args: ["-c", "while true; do sleep 3600; done"]
              args: ["--backend", "nccl", "--batch-size", "256", "--epochs", "4", "--repeat", "1", "--num-workers", "16", "--pin-memory", "--persistent-workers", "--prefetch-factor", "2"]
              # args: ["--backend", "nccl", "--batch-size", "256", "--epochs", "4", "--repeat", "1", "--num-workers", "2", "--use-syn", "--num-syn-batches", "100"]
              resources: 
                limits:
                  nvidia.com/gpu: 1
                  rdma/rdma_shared_device_a: 1
                  memory: "128Gi"  
                  # cpu: 4
                requests:
                  nvidia.com/gpu: 1
                  rdma/rdma_shared_device_a: 1
                  memory: "128Gi"
                  cpu: 16
              command:
                # - "/bin/bash"
                - "python"
                - "/opt/workspace/src/benchmark_resnet_imagenet.py"
              env:
                - name: NCCL_DEBUG
                  value: "INFO"
                - name: NCCL_IB_PCI_RELAXED_ORDERING
                  value: "0"
                - name: NCCL_IB_DISABLE
                  value: "0"
                - name: NCCL_IB_HCA
                  value: "mlx5_0,mlx5_1,mlx5_2,mlx5_5,mlx5_6,mlx5_7,mlx5_8,mlx5_13"
              securityContext:
                capabilities:
                  add:
                    - IPC_LOCK
              volumeMounts:
                - name: shared-storage
                  mountPath: /opt/data
                - mountPath: /dev/shm
                  name: dshm
          volumes:
            - name: shared-storage
              persistentVolumeClaim:
                claimName: pvc-ml-intelliflash
            - name: dshm
              emptyDir:
                medium: Memory
                sizeLimit: "128Gi"